{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MEMORY\n",
    "랭체인에는 5가지 정도 종류의 메모리가 있는데 각자 저장 방식도 다르고 각자만의 장단점이 있다.\n",
    "`챗봇에 메모리를 추가하지 않으면 챗봇은 아무것도 기억할 수 없다``.\n",
    "유저가 자신의 이름을 말하거나 이전 질무에 이어지는 질문을 해도, 챗봇은 메모리, 즉 기억력이 없기 때문에\n",
    "대화를 이해할 수 있는 능력이 없다.\n",
    "오픈API에서 제공하는 기본 API는 랭체인 없이 사용할 수 있는데, 메모리를 지원하지 않는다.\n",
    "한마디로 stateless.\n",
    "즉, `모델에게 어떤 말을 건네면 모델은 답을 한 직후에 모든 대화 내용을 잊어버리게 된다.`\n",
    "아무런 내용도 저장하지 않는다는 말이다.\n",
    "챗GPT에는 메모리가 탑재되어 있기 때문에 실제로 어떤 사람과 얘기하고 있다는 느낌을 들게한다.\n",
    "챗봇이 이전의 대화 내용이나 질문을 기억하고 답할 수 있으므로...\n",
    "\n",
    "### 강의 내용\n",
    "일단 각 메모리의 종류를 알아보고 차이점을 살펴본 후,\n",
    "랭체인에 메모리를 탑재시키는 방법을 배워보자.\n",
    "\n",
    "## 5.0 ConversationBufferMemory\n",
    "첫번째 메모리는 `Conversation Buffer Memory`라고하는데, 이메모리는 되게 단순하다.\n",
    "그냥 단순히 이전 대화 내용 전체를 저장하는 메모리이다.\n",
    "### 단점\n",
    "대화 내용이 길어질수록 `메모리도 계속 커지므로 비효율적`이다.\n",
    "위에서 설명했듯이 `모델 자체에는 메모리`가 없다. \n",
    "그래서 우리가 모델에게 요청을 보낼 때 `이전 대화 기록 전체를 같이 보내야 된다`.\n",
    "그래야지만 모델이 전에 일어났던 대화를 보고 이해를 할 수 있다.\n",
    "유저와 AI의 대화가 길어질수록 우리가 모델에게 `매번 보내야 될 대화 기록이 길어진다`는 뜻이다.\n",
    "상당히 `비효율적`이고 당연히 `비용도 많이 발생`한다.\n",
    "\n",
    "비효율적이긴 하지만, 그래도 제일 이해하기 쉬운 메모리니까 한번 살펴보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# memory = ConversationBufferMemory()\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "# 아래와 같에 메모리에 모든 이력이 쌓이므로 대화가 길어질수록 비효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 ConversationBufferWindowMemory\n",
    "5.0 에서는 ConversationBufferMemory에 대해서 배웠다.\n",
    "ConversationBufferMemory와 다르게 ConversationBufferWindowMemory는 `대화의 특정 부분만을 저장하는 메모리`이다.\n",
    "예를 들어 가장 `최근 5개의 메시지만 저장`한다고 하면 6번째 메시지가 추가되었을 때,\n",
    "`가장 오래된 메시지는 버려지는 방식`이다.\n",
    "\n",
    "ConversationBufferWindowMemory는 `대화의 특정 부분, 저장범위는 우리가 직접 설정` 할 수 있다.\n",
    "### 장점\n",
    "메모리를 특정 크기로 유지할 수 있다는게 이메모리의 큰 장점이다.\n",
    "이 메모리의 장점은 모든 대화 내용을 저장하지 않아도 된다는 점\n",
    "### 단점\n",
    "챗봇이 `전체 대화`가 아닌 `최근 대화에만 집중`한다는 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(5, 5)\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실행 결과\n",
    "위 결과를 보면 마지막에 5번쨰 메모리를 추가하면 `첫번째 메모리 데이터가 제거`된 것을 알수 있다.\n",
    "`메모리의 사이즈가 계속 늘어나지 않는다는게 장점`이지만,\n",
    "`단점은 챗봇이 예전 대화를 기억하기 힘들다는 점`이다.\n",
    "`최근에 일어난 대화에만 집중`한다는 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 ConversationSummaryMemory\n",
    "`ConversationSummaryMemory`는 `llm`을 사용한다.\n",
    "ConversationSummaryMemory는 message 그대로 저장하는 것이 아니라\n",
    "conversation의 요약을 자체적으로 해주는 것이다.\n",
    "이 부분은 유용하다. 특히 매우 긴 Conversation이 있는 경우에.\n",
    "초반에는 ConversationSummaryMemory는 이전보다 더 많은 토큰과 저장공간을 차지하게 된다.\n",
    "왜냐하면 위에서는 'Hi', 'How are you?'와 같은 짧은 메시지만을 저장했지만,\n",
    "만약 ConversationSummaryMemory가 사람과 대화를 하고, 사람이 자신을 소개하면서 AI가 답하게 되는 경우를 생각해보자.\n",
    "그러면 더 오랜 시간이 걸릴 것이다. 초반에는 더 오래 걸릴것이다.\n",
    "그러나 conversation은 버퍼 메모리를 사용하고 있어서 대화가 진행될수록 저장된 모든 메시지가 매우 많아지면서 연결될 것이다.\n",
    "`conversation의 메시지가 많아질수록 ConversationSummaryMessage의 도움을 받아서 요약`하는 것이 `토큰의 양도 줄어들면서 훨씬 나은 방법`이 될것이다.\n",
    "\n",
    "Conversation이 많아질수록 이전처럼 모든 메세지를 버퍼에 저장하는 것 보다 Conversation을 사용하여 요약하는 것이 훨씬 낫다는 걸 알게 될 것이다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# 메모리를 llm을 이요한다. 메모리를 실행하는데 비용이 든다는 뜻이다.\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Jungwon, I live in South Korea\", \"Wow that is so cool!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Jungwon from South Korea. The AI finds this information cool and expresses a desire to visit South Korea because it is pretty.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 ConversationSummaryBufferMemory\n",
    "`ConversationSummaryMemory`, `ConversationBuffer`의 결합이다.\n",
    "이것이 하는 일은, `메모리에 보내온 메시지의 수를 저장`하는 것이다.\n",
    "또한, 우리가 limit에 다다른 순간에, 그냥 무슨 일이 일어났는지 잊어버리는 것 대신에\n",
    "오래된 메시지들을 summarize(요약)할 것이다.\n",
    "* 이것이 의미하는 바는 `가장 최근의 상호작용을 계속 추적`한다는 것이다.\n",
    "    * 가장 최근 및 가장 오래전에 주고 받은 메시지가 모두 `잊혀지지 않고 요약이 되는 것`이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150, #가능한 메시지 토큰 수의 최대값.\n",
    "    return_messages=True # 이것을 채팅 모델에 사용할 것이기 때문에 True로 하는 것이 좋다.\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Jungwon, I live in South Korea\", \"Wow that is so cool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Jungwon, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!')]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Jungwon, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!')]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Korea from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Jungwon, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Brazil from Argentina?\", \"I don't know! Suer far!\")\n",
    "\n",
    "add_message(\"What is the capital of France?\", \"Paris!\")\n",
    "\n",
    "add_message(\"What is the capital of Germany?\", \"Berlin!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Jungwon from South Korea. The AI responds by expressing admiration for their location and wishing it could go there. When asked about the distance between Korea and Argentina, the AI admits it doesn\\'t know but assumes it is \"super far.\"'),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Suer far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Suer far!\"),\n",
       "  HumanMessage(content='What is the capital of France?'),\n",
       "  AIMessage(content='Paris!'),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Suer far!\"),\n",
       "  HumanMessage(content='What is the capital of France?'),\n",
       "  AIMessage(content='Paris!'),\n",
       "  HumanMessage(content='What is the capital of Germany?'),\n",
       "  AIMessage(content='Berlin!')]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실행 결과\n",
    "위 결과 처럼 메모리에 지정된 토큰수가 넘어가는 경우 `오래된 메시지들이 요약`된다.\n",
    "`하지만, 이것은 우리가 지불하는 API를 사용하고 있다는 것.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 ConversationKGMemory (Conversation Knowledge Graph Memory)\n",
    "이것도 LLM을 사용하는 memory class이다.\n",
    "대화 중의 엔티티의 Knowledge graph를 만든다.\n",
    "`가장 중요한 것들만 뽑아내는 요약본` 같은 것이다.\n",
    "\n",
    "아래 코드는 Knowledge graph에서 히스토리를 가지고 오지 않고 엔티티를 가지고 오기 때문에 \n",
    "get_history함수를 지웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True # 이것을 채팅 모델에 사용할 것이기 때문에 True로 하는 것이 좋다.\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "# def get_history():\n",
    "#     return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi I'm Jungwon, I live in South Korea\", \"Wow that is so cool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Jungwon: Jungwon is a person. Jungwon lives in South Korea.')]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is Jungwon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Jungwon likes Kinchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Jungwon: Jungwon is a person. Jungwon lives in South Korea. Jungwon likes Kinchi.')]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"What does Jungwon like\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationTokenBufferMemory\n",
    "Interaction의 최대 값을 가지고 있는 것 대신에 token의 총 양을 계산한다.\n",
    "ConversationBufferWindow와 유사하다. 단지, Interaction 대신에 Token을 이용한다는 것.\n",
    "\n",
    "### Entity\n",
    "대화 중에 entity를 추출한다.\n",
    "주목해야할 점은 여기에 정말 많은 데이터베이스를 기반으로 한 아주 많은 integration이 있다는 것이다.\n",
    "Memory를 DB에 백업할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Memory on LLMChain\n",
    "memory를 chain에 꽂는 방법과 두 종류의 chain을 이용을 사용해서 꽂는 방법을 배우자.\n",
    "* LLM chain \n",
    "    `LLM chain`은 `off-the-shelf chain`인데 이것은 일반적인 목적을 가진 chain을 의미하고 langchain에 아주 많고 아주 유용하다.\n",
    "    하지만 우리가 직접 커스텀해서 만든 chain을 활용하기를 선호할 거다. off-the-shelf chain은 빠르게 시작할 수 있게해서 좋긴 하지만 프레임워크를 다루느라 머리 싸매거나 off-the-shelf chain을 커스텀하기보다 직접 만들고 싶을 떄 그냥 이전 배운 것 처럼`langchain expression`언어를 활용해서 우리의 것을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human: My name is Jungwon\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Jungwon! How can I assist you today?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    memory=memory, \n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True # chain에 값을 전달했을때 chain프로픔트 로그들을 확인 할수 있다.\n",
    "    )\n",
    "\n",
    "chain.predict(question=\"My name is Jungwon\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Jungwon\n",
      "AI: Hello Jungwon! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great! Seoul is a vibrant and bustling city. How can I assist you today, Jungwon?\n",
      "Human: What is my name?\n",
      "AI: Your name is Jungwon.\n",
      "    Human: I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant and bustling city. How can I assist you today, Jungwon?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    System: The human introduces themselves as Jungwon.\n",
      "AI: Hello Jungwon! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great! Seoul is a vibrant and bustling city. How can I assist you today, Jungwon?\n",
      "Human: What is my name?\n",
      "AI: Your name is Jungwon.\n",
      "Human: I live in Seoul\n",
      "AI: That's great! Seoul is a vibrant and bustling city. How can I assist you today, Jungwon?\n",
      "    Human: What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI: Your name is Jungwon.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"System: The human introduces themselves as Jungwon. The AI greets Jungwon and asks how it can assist them today.\\nHuman: I live in Seoul\\nAI: That's great! Seoul is a vibrant and bustling city. How can I assist you today, Jungwon?\\nHuman: What is my name?\\nAI: Your name is Jungwon.\\nHuman: I live in Seoul\\nAI: That's great! Seoul is a vibrant and bustling city. How can I assist you today, Jungwon?\\nHuman: What is my name?\\nAI: AI: Your name is Jungwon.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Chat Based Memory\n",
    "대화 기반 메시지의 memory는 아주 쉽다.\n",
    "기억해야할 것은 memory클래스가 memory를 두가지 방식으로 출력할 수 있다는 것을 알아야한다.\n",
    "* 문자열 형태\n",
    "* 메시지 형태\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Jungwon\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Jungwon! How can I assist you today?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True # 문저열로 바꾸지말고 실제 메세지로 바꿔달라는 의미\n",
    ")\n",
    "\n",
    "# template = \"\"\"\n",
    "#     You are a helpful AI talking to a human.\n",
    "\n",
    "#     {chat_history}\n",
    "#     Human: {question}\n",
    "#     You:\n",
    "# \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    memory=memory, \n",
    "    prompt=prompt,\n",
    "    verbose=True # chain에 값을 전달했을때 chain프로픔트 로그들을 확인 할수 있다.\n",
    "    )\n",
    "\n",
    "chain.predict(question=\"My name is Jungwon\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Jungwon\n",
      "AI: Hello Jungwon! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great! Seoul is a vibrant and bustling city. How can I help you with anything related to Seoul or any other topic you have in mind?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Jungwon\n",
      "AI: Hello Jungwon! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great! Seoul is a vibrant and bustling city. How can I help you with anything related to Seoul or any other topic you have in mind?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Jungwon.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 LCEL Based Memory\n",
    "LangCahin Expression언어를 이용하여 생성된 체인에 메모리를 추가하는 것은 어렵지 않다. \n",
    "`실제로 변경 작업을 할 떄 권장되는 방법`이다.\n",
    "현재 LangChain Expression언어에서 동작하는 방법이다. 아마도 미래에는 메모리를 추가하는게 LLM체인을 사용하는 것 만큼이나 쉬워질 것이다.\n",
    "현재로서는 아주 수동적인 과정이다. 하지만 괜찮다. 수동적이기 때문에 그리 어렵지 않다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Jungwon! How can I assist you today?')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True # 문저열로 바꾸지말고 실제 메세지로 바꿔달라는 의미\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "   \n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": \"My name is Jungwon\"})\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content })\n",
    "    print(result)\n",
    "    \n",
    "chain.invoke({\"question\": \"My name is Jungwon\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
